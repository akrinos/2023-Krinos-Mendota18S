import os
import pandas as pd 
import numpy as np
from Bio import SeqIO

input_dir = "../redo-blast-search-sep-types" #"."
blast_res_out = "blast_out"

length_dict = dict({"RF00177":750,"RF01960":1000})

rule all:
    input:
        blast_out = expand(os.path.join(input_dir, "blast_out", "{model}",
                            "blast_formatted.out"),model=["16S","18S"]),
        blast_out_fmt = expand(os.path.join(input_dir, "blast_top_1", "{model}",
                            "blast_formatted_reduced.cols.out"),model=["16S","18S"]),
        blast_out_16 = os.path.join(input_dir, "selected_fasta", "16S",
                            "16S_seq_list.txt"),
        blast_out_18 = os.path.join(input_dir, "selected_fasta", "18S",
                            "18S_seq_list.txt"),
        fasta_16 = os.path.join(input_dir, "selected_fasta", "16S", "selected_seqs.fasta"),
        fasta_18 = os.path.join(input_dir, "selected_fasta", "18S", "selected_seqs.fasta"),
        ncbi_output = expand(os.path.join(input_dir, "taxonomy_blast_hits", "{model}_tax.out"),model=["16S","18S"]),
        blast_w_tax = expand(os.path.join(input_dir, "blast_w_tax", "{model}",
                                "blast_formatted_reduced.cols.out"),model=["16S","18S"])

rule extract_model:
    input:
        infile = os.path.join("all_sequences.fasta")
    output:
        outfile = os.path.join("all_sequences_{model}.fasta")
    params:
        model = "{model}"
    run:
        if params.model == "16S":
            token_check = "bacteria"
        else:
            token_check = "eukary"
        input_seqs = list(SeqIO.parse(input.infile,"fasta"))
        all_seqs = []
        for input_seq in input_seqs:
            if token_check in input_seq.id:
                all_seqs.append(input_seq)
        with open(output.outfile, "w") as output_handle:
            SeqIO.write(all_seqs, output_handle, "fasta")

rule cluster_extracted:
    input:
        fasta_files = os.path.join("all_sequences_{model}.fasta")
    output:
        sorted = os.path.join(input_dir, "sorted_contigs", "{model}",
                            "sorted.fasta"),
        centroids = os.path.join(input_dir, "clustered_contigs", "{model}",
                            "clustered.centroids.fasta"),
        clusters = os.path.join(input_dir, "clustered_contigs", "{model}","clustered.clusters")
    log:
        err = os.path.join(input_dir,"logs","vsearch","{model}.concat.err"),
        out = os.path.join(input_dir,"logs","vsearch","{model}.concat.out")
    params:
        length_use = 750
    shell:
        '''
        vsearch --sortbylength {input.fasta_files} --output {output.sorted} --minseqlength {params.length_use} --notrunclabels 1>> {log.out} 2>> {log.err}
        vsearch --cluster_fast {output.sorted} -id 0.97 --centroids {output.centroids} --uc {output.clusters} --notrunclabels 1>> {log.out} 2>> {log.err}
        '''

rule blast_search_16S:
    input:
        db = os.path.join("16S_db","16S_ribosomal_RNA.nos"),
        query = os.path.join(input_dir, "clustered_contigs","16S",
                            "clustered.centroids.fasta")
        #query = os.path.join("../meta-lakes/2021-06-11_treebuilder/clustered_contigs/reference.RF00177.clustered.core.fasta")
        #query = os.path.join("2021-11-07_treebuilder/clustered_contigs/reference.RF00177.clustered.core.fasta")
        #RF00177.clustered.core.fasta")
        #query = os.path.join("all_sequences.fasta")
    output:
        os.path.join(input_dir, "blast_out", "16S",
                            "blast_formatted.out")
    params:
        db = os.path.join("16S_db","16S_ribosomal_RNA")
    shell:
        '''
        blastn -db {params.db} -out {output} -query {input.query} -outfmt 6 -num_threads 8 -evalue 10e-4
        '''
        
rule blast_search_18S:
    input:
        db = os.path.join("18S_db","SSU_eukaryote_rRNA.nos"),
        query = os.path.join(input_dir, "clustered_contigs","18S",
                            "clustered.centroids.fasta")
        #query = os.path.join("../meta-lakes/2021-06-11_treebuilder/clustered_contigs/reference.RF00177.clustered.core.fasta")
        #query = os.path.join("2021-11-07_treebuilder/clustered_contigs/reference.RF01960.clustered.core.fasta")
        #RF01960.clustered.core.fasta")
        #query = os.path.join("all_sequences.fasta")
    output:
        os.path.join(input_dir, "blast_out", "18S",
                            "blast_formatted.out")
    params:
        db = os.path.join("18S_db","SSU_eukaryote_rRNA")
    shell:
        '''
        blastn -db {params.db} -out {output} -query {input.query} -outfmt 6 -num_threads 8 -evalue 10e-4
        '''
        
rule select_best_hits:
    input:
        blast_in = os.path.join(input_dir, "blast_out", "{model}",
                            "blast_formatted.out")
    output:
        blast_out = os.path.join(input_dir, "blast_top_1", "{model}",
                            "blast_formatted_reduced.cols.out")
    params:
        model = "{model}"
    run:
        blast_file = pd.read_csv(input.blast_in, sep = "\t", header=None,
                         names=["qseqid","sseqid","pident","length","mismatch",
                                "gapopen","qstart","qend","sstart","send","evalue","bitscore"])
        if params.model == "16S":
            blast_file = blast_file[["bacteria" in curr for curr in blast_file.qseqid]]
        else:
            blast_file = blast_file[["eukary" in curr for curr in blast_file.qseqid]]
        blast_file = blast_file.reset_index(drop=True)
        blast_file["length"] = blast_file["length"].astype(float)
        blast_file = blast_file.loc[blast_file["length"] > 100,:]
        blast_file["pident"] = blast_file["pident"].astype(float)
        blast_file = blast_file.loc[blast_file["pident"] >= 75,:]
        reduced_file = blast_file.loc[blast_file.groupby("qseqid")["bitscore"].nlargest(1).reset_index(level="qseqid").index,:]
        #reduced_file = blast_file.groupby("qseqid")['bitscore'].nlargest(1).reset_index()
        
        reduced_file.to_csv(output.blast_out, sep="\t")
        
rule select_best_model:
    input:
        blast_16 = os.path.join(input_dir, "blast_out", "16S",
                            "blast_formatted.out"),
        blast_18 = os.path.join(input_dir, "blast_out", "18S",
                            "blast_formatted.out")
    output:
        blast_out_16 = os.path.join(input_dir, "selected_fasta", "16S",
                            "16S_seq_list.txt"),
        blast_out_18 = os.path.join(input_dir, "selected_fasta", "18S",
                            "18S_seq_list.txt"),
        fasta_16 = os.path.join(input_dir, "selected_fasta", "16S", "selected_seqs.fasta"),
        fasta_18 = os.path.join(input_dir, "selected_fasta", "18S", "selected_seqs.fasta")
    run:
        blast_16 = pd.read_csv(input.blast_16, sep = "\t", header=None,
                         names=["qseqid","sseqid","pident","length","mismatch",
                                "gapopen","qstart","qend","sstart","send","evalue","bitscore"])
        blast_16["length"] = blast_16["length"].astype(float)
        blast_16 = blast_16.loc[blast_16["length"] > 100,:]
        blast_18 = pd.read_csv(input.blast_18, sep = "\t", header=None,
                         names=["qseqid","sseqid","pident","length","mismatch",
                                "gapopen","qstart","qend","sstart","send","evalue","bitscore"])
        blast_18["length"] = blast_18["length"].astype(float)
        blast_18 = blast_18.loc[blast_18["length"] > 100,:]
        blast_16 = blast_16[["bacteria" in curr for curr in blast_16.qseqid]]
        blast_18 = blast_18[["eukary" in curr for curr in blast_18.qseqid]]
        
        blast_16 = blast_16.reset_index(drop=True)
        blast_16 = blast_16.loc[blast_16.groupby("qseqid")["bitscore"].nlargest(1).reset_index(level="qseqid").index,:]
        blast_18 = blast_18.reset_index(drop=True)
        blast_18 = blast_18.loc[blast_18.groupby("qseqid")["bitscore"].nlargest(1).reset_index(level="qseqid").index,:]
        
        blast_16.columns = [curr + "_16" for curr in blast_16.columns]
        blast_18.columns = [curr + "_18" for curr in blast_18.columns]
        
        blast_16["SeqID"] = [curr.split("SSU_")[0] for curr in blast_16.qseqid_16]
        blast_18["SeqID"] = [curr.split("SSU_")[0] for curr in blast_18.qseqid_18]
       
        merged_blast = pd.merge(blast_16,blast_18, how = "outer").fillna(0)
        
        good_16_inds = [ind for curr16,curr18,ind in zip(merged_blast.bitscore_16,merged_blast.bitscore_18,merged_blast.index) if (curr16 > 0) and ((curr18 / curr16) < 0.50)]
        good_18_inds = [ind for curr16,curr18,ind in zip(merged_blast.bitscore_16,merged_blast.bitscore_18,merged_blast.index) if (curr18 > 0) and ((curr16 / curr18) < 0.50)]
        
        merged_blast_16 = merged_blast.loc[good_16_inds,:]
        merged_blast_18 = merged_blast.loc[good_18_inds,:]
        list_16_ids = list(merged_blast_16.qseqid_16)
        list_18_ids = list(merged_blast_18.qseqid_18)
        
        with open(output.blast_out_16, "w") as f:
            f.writelines(list_16_ids)
        with open(output.blast_out_18, "w") as f:
            f.writelines(list_18_ids)
        
        #with open("all_sequences.fasta") as input_file:
        #    iterator_1 = SeqIO.parse(input_file, "fasta")
        records_16 = [r for r in SeqIO.parse("all_sequences.fasta", "fasta") if r.id in list(list_16_ids)]
        records_18 = [r for r in SeqIO.parse("all_sequences.fasta", "fasta") if r.id in list(list_18_ids)]
        #with open("all_sequences.fasta") as input_again:
        #    iterator_2 = SeqIO.parse(input_again, "fasta")
        #    records_18 = [r for r in iterator_2 if r.id in list(list_18_ids)]
            
        ## WRITE 16S FASTA FILE ##
        with open(output.fasta_16, "w") as output_handle:
            SeqIO.write(records_16, output_handle, "fasta")
            
        ## WRITE 18S FASTA FILE ##
        with open(output.fasta_18, "w") as output_handle:
            SeqIO.write(records_18, output_handle, "fasta")
            
rule readout_tax_ids:
    input:
        blast_out = os.path.join(input_dir, "blast_top_1", "{model}",
                                "blast_formatted_reduced.cols.out")
    output:
        ncbi_ref_seq_id = expand(os.path.join(input_dir, "blast_top_1", "split_files", "{model}_{curr}_refseqs.txt"), model = "{model}", curr = list(range(0,40)))
    run:
        blast_file = pd.read_csv(input.blast_out, sep = "\t", header=None,
                                 names=["qseqid","sseqid","pident","length","mismatch",
                                        "gapopen","qstart","qend","sstart","send","evalue","bitscore"])
        blast_file = blast_file.drop_duplicates(subset="sseqid")
        ctr1=0
        ctr2=int(len(blast_file.index)/len(output.ncbi_ref_seq_id))
        for curr_file in output.ncbi_ref_seq_id:
            with open(curr_file, "w") as f:
                f.write(",".join([curr.split(".")[0] for curr in list(blast_file.sseqid)[ctr1:ctr2]]))
            ctr1=ctr2
            ctr2=ctr1+int(len(blast_file.index)/len(output.ncbi_ref_seq_id))
            if ctr2 > len(blast_file.index):
                ctr2 = len(blast_file.index)
            
rule get_refseq_taxid:
    input:
        ncbi_ref_seq_id = expand(os.path.join(input_dir, "blast_top_1", "split_files", "{model}_{curr}_refseqs.txt"), model = "{model}", curr = list(range(0,40)))
    output:
        ncbi_output = os.path.join(input_dir, "taxonomy_blast_hits", "{model}_tax.out")
    shell:
        '''
        echo "" > {output.ncbi_output}
        export NCBI_API_KEY=697b834f619e7a862c92cf01f382b4a45308
        for ncbi_ref_file in {input.ncbi_ref_seq_id}; do
            ncbi_refids=$(cat $ncbi_ref_file | tr "," "\n")
            for ncbi_refid in $ncbi_refids; do
                if [[ $ncbi_refid != "sseqid" ]]; then
                    taxid=$(efetch -db nucleotide -id $ncbi_refid -format docsum | xtract -pattern DocumentSummary -element TaxId) || true
                    lineage=$(efetch -db taxonomy -id $taxid -format xml | xtract -pattern TaxaSet -element Lineage) || true
                    echo "$ncbi_refid, $taxid, $lineage" >> {output.ncbi_output}
                 fi
            done
        done
        '''
#taxids=$(efetch -db nucleotide -id $ncbi_refids -format docsum | xtract -pattern DocumentSummary -element TaxId | efetch -db taxonomy -id) 
#efetch -db taxonomy -id $taxids -format xml | xtract -pattern TaxaSet -element Lineage
#resulting_table=$(efetch -db nucleotide -id $ncbi_refids -format xml | xtract -pattern Seq-entry -block Bioseq -element Textseq-id_accession,BinomialOrgName_genus,RNA-ref_ext_name,User-field_data_strs_E,OrgName_lineage)
#echo $resulting_table >> {output.ncbi_output}
#resulting_table = pd.read_csv(input.ncbi_output, sep = "\t", header=None,
#                                 names=["NR_accession","Genus","RNAtype","ProjectNumber","Lineage"])

rule combine_hits:
    input:
        ncbi_output = os.path.join(input_dir, "taxonomy_blast_hits", "{model}_tax.out"),
        blast_out = os.path.join(input_dir, "blast_top_1", "{model}",
                                "blast_formatted_reduced.cols.out")
    output:
        blast_w_tax = os.path.join(input_dir, "blast_w_tax", "{model}",
                                "blast_formatted_reduced.cols.out")
    run:
        blast_file = pd.read_csv(input.blast_out, sep = "\t")
        blast_file["sseqid"] = [str(curr.split(".")[0]) for curr in blast_file["sseqid"]]
        resulting_table = pd.read_csv(input.ncbi_output, sep = ",", header=None,
                                 names=["NR_accession","TaxID","Lineage"])
        resulting_table["NR_accession"] = [str(curr) for curr in resulting_table["NR_accession"]]
        merged_table = pd.merge(blast_file,resulting_table,how="left",left_on="sseqid",right_on="NR_accession")
        merged_table.to_csv(output.blast_w_tax, sep="\t")
